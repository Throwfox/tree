{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data into schemaRDD using HiveContext\n",
    "   Because many times our data will come from relational data structures which we can view in [Hive](https://hive.apache.org/) or [Impala](http://impala.io/index.html), we will start our analysis in the same way. We can do this using one of the two contexts which are exposed when we start pySpark. The first, ***sc***, is a [pyspark.SparkContext](https://spark.apache.org/docs/1.3.0/api/python/pyspark.html?highlight=sparkcontext#pyspark.SparkContext) which is main entry point into spark functionality and represents the connection to our spark cluster. The second, ***sqlCtx***, is a [pyspark.HiveContext](https://spark.apache.org/docs/1.3.0/api/python/pyspark.sql.html?highlight=hivecontext#pyspark.sql.HiveContext) which will be our main entry point to spark SQL functionality. It is Hive enabled so we can write and evaluate queries which are already loaded into Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = sqlCtx.sql('SELECT value, acct_bal, age FROM german_parquet limit 10')\n",
    "dat.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The german credit data was loaded during the Vagrant up process. It is a popular dataset for binary classification with categorical data. Since the original data set was in German, we are providing comperable engligh column names, identify if the field is continuous or categorical, and the number of categories if categorical. If you are interested, you can check out what each of the categories are [here](http://www.statistik.lmu.de/service/datenarchiv/kredit/kreditvar_e.html).\n",
    "\n",
    "| Field        | Descripion                        | Cont. | Cat. | NumCat |\n",
    "| ------------ | --------------------------------- |:-----:|:----:|:------:|\n",
    "| cred         | Creditability (Label)             | n/a   | n/a  | n/a    |\n",
    "| acct_bal     | Balance of current account        |       | X    | 4      |\n",
    "| dur_cred     | Duration of Credit (months)       | X     |      |        |\n",
    "| pay_stat     | Payment Status of Previous Credit | X     |      |        |\n",
    "| purpose      | Purpose                           | X     |      |        |\n",
    "| cred_amt     | Credit Amount                     | X     |      |        |\n",
    "| value        | Value Savings/Stocks              |       | X    | 5      |\n",
    "| len_emp      | Length of current employment      |       | X    | 5      |\n",
    "| install_pc   | Instalment per cent               |       | X    | 4      |\n",
    "| sex_married  | Sex & Marital Status              |       | X    | 4      |\n",
    "| guarantors   | Guarantors                        |       | X    | 3      |\n",
    "| dur_addr     | Duration in Current address       |       | X    | 4      |\n",
    "| max_val      | Most valuable available asset     |       | X    | 4      |\n",
    "| age          | Age (years)                       | X     |      |        |\n",
    "| concurr      | Concurrent Credits                |       | X    | 3      |\n",
    "| typ_aprtmnt  | Type of apartment                 |       | X    | 3      |\n",
    "| no_creds     | No of Credits at this Bank        |       | X    | 4      |\n",
    "| occupation   | Occupation                        |       | X    | 4      |\n",
    "| no_dep       | No of dependents                  |       | X    | 2      |\n",
    "| telephone    | Telephone                         |       | X    | 2      |\n",
    "| foreign_wkr  | Foreign Worker                    |       | X    | 2      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will be working with all data in the german table, we will reassign the [DataFrame](https://spark.apache.org/docs/1.3.0/api/python/pyspark.sql.html?highlight=dataframe#pyspark.sql.DataFrame) variable, dat, to include the entire data set. We can also take a look at the metadata and see it matches above. It's really convenient to be able to reuse metadata already defined in the Hive matastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = sqlCtx.sql('SELECT * FROM german_parquet')\n",
    "dat.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning & formatting\n",
    "   Our data source was fortunately well documented with fully populated records (this will not always be the case). However, there are two modifications that we will consistently need to make to data to prepare it for MLlib functions:\n",
    "#### Modify field data and type\n",
    "   More specifically make sure that the fields are converted into a form that is expected by Spark MLlib. In our example we will be running a classification decision tree where many of the fields are categorical. The standard form expected by mllib.tree is that the index starts at zero. Further we need to be sure that fields consistently map to the same number to ensure that the model is being applied appropriately. Luckily, this category to number mapping was already done for use with a small exception; in some fields the categories increment starting from 1 however, mllib.tree expects them to increment from 0.\n",
    "#### Modify row type\n",
    "   This row type, is really to put data into an object that is serializable and performs well. Additionally, the row type inherently identifies which column is considered the response or 'label' for the record, which is necessary information when working with supervised learning algorithms. The row form is called a [LabeledPoint](https://spark.apache.org/docs/1.3.0/api/python/pyspark.mllib.html?highlight=labeledpoint#pyspark.mllib.regression.LabeledPoint). The expected RDD used in MLlib is an RDD of LabeledPoints.\n",
    "\n",
    "#### A transform function for LabeledPoints\n",
    "   Since we know that we will frequently want our data to be used in analysis, we can create a function which maps our data into the appropriate form. Fortunately, both modifications above can be done in a simple map of rows from one form to another. This might not always be the case, and there could be instances where aggregation is done to data in the form of a reduction. We will not explore that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "def german_lp(x):\n",
    "    vals=x.asDict()\n",
    "    label=vals['cred']\n",
    "    feats=[vals['acct_bal']-1,\n",
    "           vals['dur_cred'],\n",
    "           vals['pay_stat'],\n",
    "           vals['purpose'],\n",
    "           vals['cred_amt'],\n",
    "           vals['value']-1,\n",
    "           vals['len_emp']-1,\n",
    "           vals['install_pc']-1,\n",
    "           vals['sex_married']-1,\n",
    "           vals['guarantors']-1,\n",
    "           vals['dur_addr']-1,\n",
    "           vals['max_val']-1,\n",
    "           vals['age'],\n",
    "           vals['concurr']-1,\n",
    "           vals['typ_aprtmnt']-1,\n",
    "           vals['no_creds']-1,\n",
    "           vals['occupation']-1,\n",
    "           vals['no_dep']-1,\n",
    "           vals['telephone']-1,\n",
    "           vals['foreign_wkr']-1]\n",
    "    return LabeledPoint(label, feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managed Seperately as an [dict](https://docs.python.org/2/library/stdtypes.html#mapping-types-dict), we need to identify which columns are categorical data and which are continuous. The dict will include all the featuress which are categorical data mapping to the number of categories are in that feature. Continuous features are identified by their omission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "german_cfi = {0:4,5:5,6:5,7:4,8:4,9:3,10:4,11:4,13:3,14:3,15:4,16:4,17:2,18:2,19:2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our function defined, it is now a simple one liner to convert our DataFrame, dat, into an RDD of Labeled Points suitable for MLlib. We will use the map function. Notice, we cache() the change. This will retain a copy of the rdd in lp form into memory. This will make iterative evaluations more performent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lp = dat.map(german_lp).cache()\n",
    "lp.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training a Decision Tree Model\n",
    "   MLlib has enhamcements may come with any version. Be sure to check the online documentation between upgrades for improvements. The documentation for model we will evaluate today can be found in [Apache Spark Documentation](https://spark.apache.org/docs/1.3.0/api/python/pyspark.mllib.html#module-pyspark.mllib.tree). From there we find the following parameters for training a DecisionTreeModel:\n",
    "\n",
    "| Parameter                         | Descripion                                                                                                         |\n",
    "| ----------------------------- | ------------------------------------------------------------------------------------------------------------------ |\n",
    "| ***data***                    | Training data: RDD of LabeledPoint. Labels are integers {0,1,...,numClasses}.                                      |\n",
    "| ***numClasses***              | Number of classes for classification.                                                                              |\n",
    "| ***categoricalFeaturesInfo*** | Map from categorical feature index to number of categories. Any feature not in this map is treated as continuous.  |\n",
    "| ***impurity***                | Supported values: “entropy” or “gini”                                                                              |\n",
    "| ***maxDepth***                | Max depth of tree. E.g., depth 0 means 1 leaf node. Depth 1 means 1 internal node + 2 leaf nodes.                  |\n",
    "| ***maxBins***                 | Number of bins used for finding splits at each node.                                                               |\n",
    "| ***minInstancesPerNode***     | Min number of instances required at child nodes to create the parent split                                         |\n",
    "| ***minInfoGain***             | Min info gain required to create a split                                                                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "model = DecisionTree.trainClassifier(lp, numClasses=2,\n",
    "                                     categoricalFeaturesInfo=german_cfi,\n",
    "                                     impurity='gini',\n",
    "                                     maxDepth=3, \n",
    "                                     maxBins=5)\n",
    "model.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is nice to be able to view the logic in a model, this text form doesn't show well how a decision tree works. Unfortunately, there is no MLlib Decision Tree plot functionality that I am aware of. For those still learning what a decision tree model is, we'll run this data set through an existing local in memory decision tree library just to view the visualization. NOTE w/ DISCLAIMER: the following snippet does not handel categorical variables as desired and will return a different model, this plot is only provided to visualize what a decision tree model does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.display import Image\n",
    "import StringIO, pydot\n",
    "%matplotlib inline\n",
    "\n",
    "X=lp.map(lambda x: np.array(x.features)).collect()\n",
    "y=lp.map(lambda x: np.array(x.label)).collect()\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, max_leaf_nodes=5)\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "# Visualize tree\n",
    "dot_data = StringIO.StringIO()\n",
    "export_graphviz(clf, out_file=dot_data)\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returing to our MLlib model. Once our model is fit, it can easily be used to predict against a data set that is already in Labeled Point form. However, with all the parameters that are available, how are we sure that we've picked the right model. We are going to evaluate using a training and validation set. Then by iterating through our parameters and different samples, we'll discover which settings have good predictive properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_dat = pd.DataFrame(columns=('obs', 'max_depth', 'num_nodes', 'train_err', 'val_err'))\n",
    "\n",
    "for obs in range(30):\n",
    "  for max_depth in range(2,8):\n",
    "    (lp_train, lp_val) = lp.randomSplit([0.8, 0.2])\n",
    "    model = DecisionTree.trainClassifier(lp_train, numClasses=2,\n",
    "                                         categoricalFeaturesInfo=german_cfi,\n",
    "                                         impurity='gini',\n",
    "                                         minInstancesPerNode=10,\n",
    "                                         maxDepth=max_depth)\n",
    "    te = lp_train.map(lambda lp: lp.label).zip(model.predict(lp_train.map(lambda lp: lp.features)))\n",
    "    ve = lp_val.map(lambda lp: lp.label).zip(model.predict(lp_val.map(lambda lp: lp.features)))\n",
    "    train_err = te.filter(lambda (v, p): v != p).count() / float(te.count())\n",
    "    val_err = ve.filter(lambda (v, p): v != p).count() / float(ve.count())\n",
    "    val_dat.loc[len(val_dat)] = [obs,model.depth(),model.numNodes(),train_err,val_err]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the complexity (number of nodes in a model) doesn't necessarily improve the error in the validation data set. By running this over very many sample data splits, averaging and splining, we can get to a representative graphic for the effects of complexity on training and validation errors. Specifically, up until approximately 23 nodes we tend to see a reduction in error which is good. However, after approximately 23 we tend to see an increase in error which is bad. This phenomenon of the validation error increasing with model complexity is called '[over fitting](https://en.wikipedia.org/wiki/Overfitting#Machine_learning)'.The idea is that the model has become to specific to the training data and contains detail not general to the population. We must be conscious of this risk when selecting our models. Understanding this effect will help us refine our training parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import UnivariateSpline\n",
    "vd_nodes=val_dat.groupby(['num_nodes'])['train_err','val_err'].median()\n",
    "vd_depth=val_dat.groupby(['max_depth'])['train_err','val_err'].median()\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(16,4)\n",
    "plot_vdn = fig.add_subplot(1,2,1)\n",
    "plot_vdd = fig.add_subplot(1,2,2)\n",
    "\n",
    "vdn_y  = vd_nodes.index.values\n",
    "vdn_tx = vd_nodes.loc[:,'train_err']\n",
    "vdn_vx = vd_nodes.loc[:,'val_err']\n",
    "\n",
    "vdn_xs = np.linspace(0, 90, 1000)\n",
    "\n",
    "#plt.plot(vdn_y, vdn_tx, 'bo', ms=5)\n",
    "vdnt_spl = UnivariateSpline(vdn_y,vdn_tx)\n",
    "vdnt_spl.set_smoothing_factor(0.5)\n",
    "plot_vdn.plot(vdn_xs, vdnt_spl(vdn_xs), 'b', lw=3)\n",
    "\n",
    "#plt.plot(vdn_y, vdn_vx, 'ro', ms=5)\n",
    "vdnv_spl = UnivariateSpline(vdn_y,vdn_vx)\n",
    "vdnv_spl.set_smoothing_factor(0.5)\n",
    "plot_vdn.plot(vdn_xs, vdnv_spl(vdn_xs), 'r', lw=3)\n",
    "plot_vdn.axis([0, 70, 0.2,0.30])\n",
    "plot_vdn.set_xlabel('Number of Nodes')\n",
    "plot_vdn.set_ylabel('Model Error')\n",
    "plot_vdn.set_title('Training & Validation Model Error Vs. Number Nodes')\n",
    "\n",
    "vdd_y  = vd_depth.index.values\n",
    "vdd_tx = vd_depth.loc[:,'train_err']\n",
    "vdd_vx = vd_depth.loc[:,'val_err']\n",
    "\n",
    "vdd_xs = np.linspace(0, 7, 500)\n",
    "\n",
    "#plt.plot(vdn_y, vdn_tx, 'bo', ms=5)\n",
    "vddt_spl = UnivariateSpline(vdd_y,vdd_tx)\n",
    "vddt_spl.set_smoothing_factor(0.5)\n",
    "plot_vdd.plot(vdd_xs, vddt_spl(vdd_xs), 'b', lw=3)\n",
    "\n",
    "#plt.plot(vdn_y, vdn_vx, 'ro', ms=5)\n",
    "vddv_spl = UnivariateSpline(vdd_y,vdd_vx)\n",
    "vddv_spl.set_smoothing_factor(0.5)\n",
    "plot_vdd.plot(vdd_xs, vddv_spl(vdd_xs), 'r', lw=3)\n",
    "plot_vdd.axis([2, 6, 0.2,0.30])\n",
    "plot_vdd.set_xlabel('Parameter maxDepth')\n",
    "plot_vdd.set_ylabel('Model Error')\n",
    "plot_vdd.set_title('Training & Validation Model Error Vs. Parameter maxDepth')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
